# LLM Agent åŠŸèƒ½è®¾è®¡æ–‡æ¡£

> ç›®çš„ï¼šæ·»åŠ å¤§æ¨¡å‹é…ç½®ã€MCP å·¥å…·é›†æˆå’ŒåŸºäºæ„å›¾è¯†åˆ«çš„æ™ºèƒ½æ“ä½œåŠŸèƒ½

---

## ä¸€ã€åŠŸèƒ½æ¦‚è¿°

### 1.1 æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½ | è¯´æ˜ |
|------|------|
| **LLM é…ç½®** | æ”¯æŒé…ç½® Base URLã€API Keyã€Modelï¼ŒModel åˆ—è¡¨è‡ªåŠ¨ä» API æ‹‰å– |
| **MCP é…ç½®** | æ”¯æŒé…ç½® MCP Serverï¼Œæ‰©å±• Agent èƒ½åŠ›ï¼ˆæ–‡ä»¶æ“ä½œã€ç½‘é¡µæµè§ˆç­‰ï¼‰ |
| **Agent å¿«æ·é”®** | é»˜è®¤ Tab é”®ï¼Œé•¿æŒ‰åè¯­éŸ³è¯†åˆ« â†’ LLM æ„å›¾ç†è§£ â†’ è°ƒç”¨ MCP å·¥å…·æ‰§è¡Œ |

### 1.2 ç”¨æˆ·äº¤äº’æµç¨‹

```
ç”¨æˆ·é•¿æŒ‰ Tab é”® â†’ å¼€å§‹å½•éŸ³ â†’ é‡Šæ”¾ â†’ è¯­éŸ³è¯†åˆ« â†’
å‘é€ç»™ LLMï¼ˆå¸¦ MCP å·¥å…·åˆ—è¡¨ï¼‰â†’ LLM å†³å®šè°ƒç”¨å“ªä¸ªå·¥å…· â†’ æ‰§è¡Œ
```

---

## äºŒã€é…ç½®è®¾è®¡

### 2.1 é…ç½®ç»“æ„ (config.yaml)

```yaml
core:
  # ... ç°æœ‰é…ç½® ...

  # LLM Agent é…ç½®
  llm_agent:
    enabled: true
    hotkey: "tab"              # è§¦å‘å¿«æ·é”®
    hotkey_hold_time: 0.5      # é•¿æŒ‰è§¦å‘æ—¶é—´

# LLM å¼•æ“é…ç½®ï¼ˆç‹¬ç«‹äºè¯­éŸ³è¯†åˆ«å¼•æ“ï¼‰
llm:
  provider: "openai"           # openai / ollama / custom

  openai:
    api_key: ""
    base_url: "https://api.openai.com/v1"
    model: "gpt-4o-mini"       # ä» API è‡ªåŠ¨æ‹‰å–å¯é€‰åˆ—è¡¨

  ollama:
    base_url: "http://localhost:11434"
    model: "llama3.2"

  custom:
    api_key: ""
    base_url: ""
    model: ""

# MCP Server é…ç½®
mcp:
  servers:
    # å†…ç½®æ–‡ä»¶ç³»ç»ŸæœåŠ¡
    filesystem:
      enabled: true
      command: "npx"
      args: ["-y", "@anthropic/mcp-filesystem", "/home/user"]

    # å†…ç½®æµè§ˆå™¨æœåŠ¡
    browser:
      enabled: true
      command: "npx"
      args: ["-y", "@anthropic/mcp-browser"]

    # è‡ªå®šä¹‰æœåŠ¡ç¤ºä¾‹
    # custom_server:
    #   enabled: false
    #   command: "python"
    #   args: ["-m", "my_mcp_server"]
    #   env:
    #     API_KEY: "xxx"
```

### 2.2 è®¾ç½®ç•Œé¢è®¾è®¡

åœ¨ SettingsDialog ä¸­æ–°å¢ **"LLM Agent"** é¡µé¢ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Agent è®¾ç½®                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”€â”€ Agent å¿«æ·é”® â”€â”€                                          â”‚
â”‚  [âœ“] å¯ç”¨ LLM Agent                                          â”‚
â”‚  å¿«æ·é”®:          [ Tab        â–¼ ]                           â”‚
â”‚  é•¿æŒ‰æ—¶é—´:        [ 0.5 ] ç§’                                  â”‚
â”‚                                                              â”‚
â”‚  â”€â”€ LLM é…ç½® â”€â”€                                              â”‚
â”‚  æœåŠ¡æä¾›å•†:      [ OpenAI å…¼å®¹ â–¼ ]                           â”‚
â”‚  Base URL:        [ https://api.openai.com/v1    ]           â”‚
â”‚  API Key:         [ â—â—â—â—â—â—â—â—â—â—â—â—                 ]           â”‚
â”‚  æ¨¡å‹:            [ gpt-4o-mini    â–¼ ] [ åˆ·æ–° ]              â”‚
â”‚                                                              â”‚
â”‚  â”€â”€ MCP Server é…ç½® â”€â”€                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ [âœ“] filesystem    /home/user              [ ç¼–è¾‘ ] â”‚    â”‚
â”‚  â”‚ [âœ“] browser       @anthropic/mcp-browser  [ ç¼–è¾‘ ] â”‚    â”‚
â”‚  â”‚ [ ] fetch         @anthropic/mcp-fetch    [ ç¼–è¾‘ ] â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            [ + æ·»åŠ  MCP Server ]             â”‚
â”‚                                                              â”‚
â”‚                                        [ ä¿å­˜ ]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 MCP Server ç¼–è¾‘å¯¹è¯æ¡†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¼–è¾‘ MCP Server                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  åç§°:            [ filesystem                   ]           â”‚
â”‚  å‘½ä»¤:            [ npx                          ]           â”‚
â”‚  å‚æ•°:            [ -y @anthropic/mcp-filesystem ]           â”‚
â”‚                   [ /home/user                   ]           â”‚
â”‚                                        [ + æ·»åŠ å‚æ•° ]        â”‚
â”‚                                                              â”‚
â”‚  ç¯å¢ƒå˜é‡:                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ KEY          â”‚ VALUE                  â”‚                  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚  â”‚ PATH         â”‚ /usr/bin               â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                        [ + æ·»åŠ å˜é‡ ]        â”‚
â”‚                                                              â”‚
â”‚                     [ æµ‹è¯•è¿æ¥ ]  [ å–æ¶ˆ ]  [ ç¡®å®š ]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ä¸‰ã€æ¨¡å‹åˆ—è¡¨è‡ªåŠ¨æ‹‰å–

### 3.1 OpenAI å…¼å®¹ API

```python
# GET /v1/models
async def fetch_models(base_url: str, api_key: str) -> list[str]:
    """ä» OpenAI å…¼å®¹ API è·å–æ¨¡å‹åˆ—è¡¨"""
    url = f"{base_url.rstrip('/')}/models"
    headers = {"Authorization": f"Bearer {api_key}"}

    async with aiohttp.ClientSession() as session:
        async with session.get(url, headers=headers) as resp:
            if resp.status == 200:
                data = await resp.json()
                # è¿‡æ»¤å‡º chat æ¨¡å‹
                models = [m["id"] for m in data.get("data", [])]
                return sorted(models)
            return []
```

### 3.2 Ollama API

```python
# GET /api/tags
async def fetch_ollama_models(base_url: str) -> list[str]:
    """ä» Ollama è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨"""
    url = f"{base_url.rstrip('/')}/api/tags"

    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            if resp.status == 200:
                data = await resp.json()
                return [m["name"] for m in data.get("models", [])]
            return []
```

### 3.3 UI åˆ·æ–°é€»è¾‘

```python
class LLMAgentPage(SettingsPage):
    def _on_refresh_models_clicked(self):
        """ç‚¹å‡»åˆ·æ–°æŒ‰é’®æ—¶æ‹‰å–æ¨¡å‹åˆ—è¡¨"""
        base_url = self.base_url_input.text()
        api_key = self.api_key_input.text()
        provider = self.provider_combo.currentData()

        # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
        self.model_combo.clear()
        self.model_combo.addItem("åŠ è½½ä¸­...")
        self.refresh_btn.setEnabled(False)

        # å¼‚æ­¥æ‹‰å–
        self._fetch_models_async(provider, base_url, api_key)

    def _on_models_fetched(self, models: list[str]):
        """æ¨¡å‹åˆ—è¡¨æ‹‰å–å®Œæˆ"""
        self.model_combo.clear()
        if models:
            self.model_combo.addItems(models)
        else:
            self.model_combo.addItem("(æ— å¯ç”¨æ¨¡å‹)")
        self.refresh_btn.setEnabled(True)
```

---

## å››ã€MCP å·¥å…·é›†æˆ

### 4.1 MCP åè®®ç®€ä»‹

MCP (Model Context Protocol) æ˜¯ Anthropic æ¨å‡ºçš„å¼€æ”¾åè®®ï¼Œå…è®¸ LLM è¿æ¥å¤–éƒ¨å·¥å…·å’Œæ•°æ®æºã€‚

é€šè¿‡ `langchain-mcp-adapters` åº“ï¼Œå¯ä»¥å°† MCP Server æä¾›çš„å·¥å…·æ— ç¼è½¬æ¢ä¸º LangChain Toolsã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LangChain Agent                       â”‚
â”‚                           â”‚                              â”‚
â”‚                           â–¼                              â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚            â”‚ langchain-mcp-adapters   â”‚                  â”‚
â”‚            â”‚   (load_mcp_tools)       â”‚                  â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â–¼              â–¼              â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚filesystemâ”‚  â”‚ browser  â”‚  â”‚  fetch   â”‚
       â”‚  Server  â”‚  â”‚  Server  â”‚  â”‚  Server  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 ä½¿ç”¨ langchain-mcp-adapters

```python
from langchain_mcp_adapters.tools import load_mcp_tools

# åŠ è½½ MCP Server æä¾›çš„å·¥å…·
tools = await load_mcp_tools(
    command="npx",
    args=["-y", "@anthropic/mcp-filesystem", "/home/user"],
)

# tools æ˜¯æ ‡å‡†çš„ LangChain Tools åˆ—è¡¨ï¼Œå¯ç›´æ¥ç”¨äº Agent
```

### 4.3 å¸¸ç”¨ MCP Server

| Server | åŠŸèƒ½ | é…ç½®ç¤ºä¾‹ |
|--------|------|----------|
| `@anthropic/mcp-filesystem` | æ–‡ä»¶è¯»å†™ã€ç›®å½•æ“ä½œ | `command: npx`, `args: [-y, @anthropic/mcp-filesystem, /home/user]` |
| `@anthropic/mcp-browser` | æµè§ˆå™¨æ§åˆ¶ | `command: npx`, `args: [-y, @anthropic/mcp-browser]` |
| `@anthropic/mcp-fetch` | HTTP è¯·æ±‚ | `command: npx`, `args: [-y, @anthropic/mcp-fetch]` |
| `@anthropic/mcp-github` | GitHub æ“ä½œ | `command: npx`, `args: [-y, @anthropic/mcp-github]` |
| `@anthropic/mcp-puppeteer` | ç½‘é¡µè‡ªåŠ¨åŒ– | `command: npx`, `args: [-y, @anthropic/mcp-puppeteer]` |

### 4.4 è‡ªå®šä¹‰ MCP Server

ç”¨æˆ·ä¹Ÿå¯ä»¥æ·»åŠ è‡ªå®šä¹‰ MCP Serverï¼š

```yaml
mcp:
  servers:
    my_custom_server:
      enabled: true
      command: "python"
      args: ["-m", "my_mcp_server"]
      env:
        API_KEY: "xxx"
```

---

## äº”ã€Agent æµ®çª—ç•Œé¢ï¼ˆå¤ç”¨ FloatingWindowï¼‰

### 5.1 è®¾è®¡ç†å¿µ

å¤ç”¨ç°æœ‰çš„ FloatingWindowï¼Œä¿æŒ"å°è€Œç²¾"çš„é£æ ¼ï¼š
- ä¿æŒç´§å‡‘çš„çª—å£å°ºå¯¸
- é«˜åº¦æ ¹æ®å†…å®¹åŠ¨æ€è°ƒæ•´ï¼ˆæœ‰ä¸Šé™ï¼‰
- ç»Ÿä¸€çš„è§†è§‰é£æ ¼
- æµç•…çš„çŠ¶æ€åˆ‡æ¢åŠ¨ç”»

### 5.2 ç•Œé¢å¸ƒå±€

**åŸºç¡€çŠ¶æ€ï¼ˆè†å¬ä¸­ï¼‰- ä¸è¯­éŸ³æ¨¡å¼ä¸€è‡´ï¼š**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚  App   â”‚   Agent Â· è†å¬ä¸­                             â”‚
â”‚  â”‚  Icon  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  â”‚   ğŸ¤   â”‚   æ­£åœ¨è†å¬...                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å¤„ç†çŠ¶æ€ï¼ˆé«˜åº¦è‡ªé€‚åº”æ‰©å±•ï¼‰ï¼š**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚  App   â”‚   Agent Â· å¤„ç†ä¸­                             â”‚
â”‚  â”‚  Icon  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  â”‚   ğŸ¤–   â”‚   ğŸ¤ å¸®æˆ‘æ‰“å¼€GitHubæœç´¢langchain             â”‚
â”‚  â”‚   â³   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â³ æ­£åœ¨æ€è€ƒ...                             â”‚
â”‚               ğŸ”§ browser.open â†’ github.com  âœ…           â”‚
â”‚               ğŸ”§ browser.type â†’ langchain   â³           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®ŒæˆçŠ¶æ€ï¼š**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚  App   â”‚   Agent Â· å®Œæˆ                               â”‚
â”‚  â”‚  Icon  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  â”‚   ğŸ¤–   â”‚   ğŸ¤ å¸®æˆ‘æ‰“å¼€GitHubæœç´¢langchain             â”‚
â”‚  â”‚   âœ…   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   âœ… å·²æ‰“å¼€GitHubå¹¶æœç´¢langchain             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 çŠ¶æ€å›¾æ ‡ï¼ˆå·¦ä¾§ Orbï¼‰

| çŠ¶æ€ | å›¾æ ‡/åŠ¨ç”» | é¢œè‰² |
|------|-----------|------|
| è†å¬ä¸­ | ğŸ¤ + è„‰åŠ¨ | è“è‰² |
| è¯†åˆ«ä¸­ | ğŸ“ + æ—‹è½¬ | è“è‰² |
| æ€è€ƒä¸­ | ğŸ¤– + â³ | ç´«è‰² |
| æ‰§è¡Œä¸­ | ğŸ¤– + ğŸ”§ | æ©™è‰² |
| å®Œæˆ | ğŸ¤– + âœ… | ç»¿è‰² |
| é”™è¯¯ | ğŸ¤– + âŒ | çº¢è‰² |

### 5.4 å³ä¾§å†…å®¹åŒºåŸŸ

```python
# æ‰©å±•ç°æœ‰çš„ FloatingWindow

class FloatingWindow(QWidget):
    # ... ç°æœ‰ä»£ç  ...

    # æ–°å¢ï¼šAgent æ¨¡å¼çš„å†…å®¹æ˜¾ç¤º
    def set_agent_content(self, content: AgentContent):
        """è®¾ç½® Agent æ¨¡å¼çš„å†…å®¹"""
        html = self._build_agent_html(content)
        self._text_area.setHtml(html)
        self._adjust_height()  # åŠ¨æ€è°ƒæ•´é«˜åº¦

    def _build_agent_html(self, content: AgentContent) -> str:
        """æ„å»º Agent å†…å®¹çš„ HTML"""
        html = []

        # ç”¨æˆ·è¾“å…¥
        if content.user_input:
            html.append(f'<div style="color:#888;">ğŸ¤ {content.user_input}</div>')
            html.append('<hr style="border:none;border-top:1px solid #333;margin:4px 0;">')

        # çŠ¶æ€/æ€è€ƒ
        if content.thinking:
            html.append(f'<div>â³ {content.thinking}</div>')

        # å·¥å…·è°ƒç”¨
        for tool in content.tool_calls:
            icon = "âœ…" if tool.status == "success" else "âŒ" if tool.status == "error" else "â³"
            html.append(f'<div style="font-size:12px;">ğŸ”§ {tool.name} â†’ {tool.summary} {icon}</div>')

        # æœ€ç»ˆç»“æœ
        if content.result:
            html.append(f'<div style="margin-top:4px;">âœ… {content.result}</div>')

        return ''.join(html)

    def _adjust_height(self):
        """æ ¹æ®å†…å®¹åŠ¨æ€è°ƒæ•´çª—å£é«˜åº¦"""
        content_height = self._text_area.document().size().height()
        # æœ€å°é«˜åº¦ 88pxï¼Œæœ€å¤§é«˜åº¦ 300px
        new_height = min(max(88, int(content_height) + 60), 300)
        self.setFixedHeight(new_height)
```

### 5.5 æ•°æ®ç»“æ„

```python
# speaky/llm/types.py

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional


class AgentStatus(Enum):
    LISTENING = "listening"      # è†å¬ä¸­
    RECOGNIZING = "recognizing"  # è¯†åˆ«ä¸­
    THINKING = "thinking"        # æ€è€ƒä¸­
    EXECUTING = "executing"      # æ‰§è¡Œå·¥å…·ä¸­
    DONE = "done"               # å®Œæˆ
    ERROR = "error"             # é”™è¯¯


@dataclass
class ToolCall:
    name: str                   # å·¥å…·åç§°
    summary: str                # å‚æ•°æ‘˜è¦ï¼ˆç®€çŸ­æ˜¾ç¤ºï¼‰
    status: str = "running"     # running / success / error


@dataclass
class AgentContent:
    user_input: str = ""                    # ç”¨æˆ·è¯­éŸ³è¾“å…¥
    thinking: str = ""                      # æ€è€ƒè¿‡ç¨‹
    tool_calls: list[ToolCall] = field(default_factory=list)
    result: str = ""                        # æœ€ç»ˆç»“æœ
    status: AgentStatus = AgentStatus.LISTENING
```

### 5.6 æµå¼æ›´æ–°é€»è¾‘

```python
# speaky/handlers/llm_agent.py

class LLMAgentHandler:
    # ...

    async def _run_agent_stream(self, text: str):
        """æµå¼è¿è¡Œ Agent å¹¶å®æ—¶æ›´æ–°æµ®çª—"""
        content = AgentContent(user_input=text, status=AgentStatus.THINKING)
        self._update_floating_window(content)

        async for event in self._llm_client.agent_executor.astream_events(
            {"input": text},
            version="v2",
        ):
            kind = event["event"]

            if kind == "on_chat_model_stream":
                # LLM æ­£åœ¨è¾“å‡º
                chunk = event["data"]["chunk"].content
                if chunk:
                    content.thinking += chunk
                    self._update_floating_window(content)

            elif kind == "on_tool_start":
                # å¼€å§‹è°ƒç”¨å·¥å…·
                tool_name = event["name"]
                tool_input = event["data"].get("input", {})
                summary = self._summarize_tool_input(tool_input)
                content.tool_calls.append(ToolCall(tool_name, summary, "running"))
                content.status = AgentStatus.EXECUTING
                self._update_floating_window(content)

            elif kind == "on_tool_end":
                # å·¥å…·è°ƒç”¨å®Œæˆ
                for tool in content.tool_calls:
                    if tool.status == "running":
                        tool.status = "success"
                        break
                self._update_floating_window(content)

            elif kind == "on_tool_error":
                # å·¥å…·è°ƒç”¨å¤±è´¥
                for tool in content.tool_calls:
                    if tool.status == "running":
                        tool.status = "error"
                        break
                self._update_floating_window(content)

        # è·å–æœ€ç»ˆç»“æœ
        content.result = await self._get_final_result()
        content.status = AgentStatus.DONE
        content.thinking = ""  # æ¸…é™¤æ€è€ƒè¿‡ç¨‹ï¼Œåªæ˜¾ç¤ºç»“æœ
        self._update_floating_window(content)

    def _update_floating_window(self, content: AgentContent):
        """æ›´æ–°æµ®çª—æ˜¾ç¤ºï¼ˆéœ€è¦åœ¨ä¸»çº¿ç¨‹æ‰§è¡Œï¼‰"""
        self._signals.agent_content.emit(content)

    def _summarize_tool_input(self, tool_input: dict) -> str:
        """ç®€åŒ–å·¥å…·å‚æ•°æ˜¾ç¤º"""
        if "url" in tool_input:
            return tool_input["url"][:30]
        if "path" in tool_input:
            return tool_input["path"][:30]
        if "query" in tool_input:
            return tool_input["query"][:20]
        return str(tool_input)[:30]
```

### 5.7 çª—å£é«˜åº¦åŠ¨ç”»

```python
# å¹³æ»‘çš„é«˜åº¦å˜åŒ–åŠ¨ç”»
def _animate_height(self, target_height: int):
    """åŠ¨ç”»è¿‡æ¸¡åˆ°ç›®æ ‡é«˜åº¦"""
    self._height_animation = QPropertyAnimation(self, b"minimumHeight")
    self._height_animation.setDuration(150)  # 150ms
    self._height_animation.setStartValue(self.height())
    self._height_animation.setEndValue(target_height)
    self._height_animation.setEasingCurve(QEasingCurve.Type.OutCubic)
    self._height_animation.start()
```

---

## å…­ã€LLM å®¢æˆ·ç«¯ï¼ˆåŸºäº LangChainï¼‰

### 6.1 ä¸ºä»€ä¹ˆä½¿ç”¨ LangChain

| ä¼˜åŠ¿ | è¯´æ˜ |
|------|------|
| **ç»Ÿä¸€æ¥å£** | æ”¯æŒ OpenAIã€Ollamaã€Anthropicã€Azure ç­‰å¤šç§ LLM |
| **Tool Calling** | å†…ç½®å·¥å…·è°ƒç”¨æ”¯æŒï¼Œè‡ªåŠ¨å¤„ç†å¤šè½®è°ƒç”¨ |
| **Agent æ¡†æ¶** | æˆç†Ÿçš„ Agent å®ç°ï¼Œæ”¯æŒ ReAct ç­‰æ¨¡å¼ |
| **MCP é›†æˆ** | é€šè¿‡ `langchain-mcp-adapters` æ— ç¼é›†æˆ MCP |
| **æµå¼è¾“å‡º** | æ”¯æŒæµå¼å“åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒ |

### 6.2 LangChain + MCP æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LangChain Agent                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ ChatOpenAI  â”‚    â”‚ ChatOllama  â”‚    â”‚ ChatAnthropicâ”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                            â–¼                                 â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                   â”‚  Agent Executor â”‚                        â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                            â–¼                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚              â”‚   MCP Tools (Adapter)   â”‚                     â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼              â–¼              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚filesystemâ”‚  â”‚ browser  â”‚  â”‚  fetch   â”‚
        â”‚  Server  â”‚  â”‚  Server  â”‚  â”‚  Server  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 LLM å®¢æˆ·ç«¯å°è£…

```python
# speaky/llm/client.py

from typing import Optional
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_mcp_adapters.tools import load_mcp_tools

from .prompts import AGENT_SYSTEM_PROMPT


class LLMClient:
    """åŸºäº LangChain çš„ LLM å®¢æˆ·ç«¯"""

    def __init__(self, config: dict):
        self.config = config
        self.provider = config.get("provider", "openai")
        self._llm = None
        self._agent_executor: Optional[AgentExecutor] = None

    def _create_llm(self):
        """æ ¹æ®é…ç½®åˆ›å»º LLM å®ä¾‹"""
        provider_config = self.config.get(self.provider, {})

        if self.provider == "openai":
            return ChatOpenAI(
                model=provider_config.get("model", "gpt-4o-mini"),
                api_key=provider_config.get("api_key"),
                base_url=provider_config.get("base_url"),
                temperature=0.7,
            )
        elif self.provider == "ollama":
            return ChatOllama(
                model=provider_config.get("model", "llama3.2"),
                base_url=provider_config.get("base_url", "http://localhost:11434"),
            )
        else:
            # è‡ªå®šä¹‰ OpenAI å…¼å®¹æ¥å£
            return ChatOpenAI(
                model=provider_config.get("model"),
                api_key=provider_config.get("api_key"),
                base_url=provider_config.get("base_url"),
                temperature=0.7,
            )

    async def initialize(self, mcp_servers: dict):
        """åˆå§‹åŒ– Agentï¼ŒåŠ è½½ MCP å·¥å…·"""
        self._llm = self._create_llm()

        # ä» MCP Server åŠ è½½å·¥å…·
        tools = []
        for name, server_config in mcp_servers.items():
            if server_config.get("enabled", False):
                try:
                    server_tools = await load_mcp_tools(
                        command=server_config["command"],
                        args=server_config.get("args", []),
                        env=server_config.get("env"),
                    )
                    tools.extend(server_tools)
                except Exception as e:
                    logger.error(f"Failed to load MCP tools from {name}: {e}")

        # åˆ›å»º Agent
        prompt = ChatPromptTemplate.from_messages([
            ("system", AGENT_SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        agent = create_tool_calling_agent(self._llm, tools, prompt)
        self._agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            max_iterations=10,
            handle_parsing_errors=True,
        )

    async def chat(self, user_message: str) -> str:
        """å‘é€æ¶ˆæ¯å¹¶è·å–å“åº”"""
        if self._agent_executor is None:
            raise RuntimeError("Agent not initialized. Call initialize() first.")

        result = await self._agent_executor.ainvoke({
            "input": user_message,
        })
        return result.get("output", "")

    async def chat_stream(self, user_message: str):
        """æµå¼å‘é€æ¶ˆæ¯"""
        if self._agent_executor is None:
            raise RuntimeError("Agent not initialized. Call initialize() first.")

        async for event in self._agent_executor.astream_events(
            {"input": user_message},
            version="v2",
        ):
            kind = event["event"]
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    yield content
```

### 6.4 æ¨¡å‹åˆ—è¡¨è·å–

```python
# speaky/llm/models.py

import aiohttp
from typing import Optional


async def fetch_openai_models(base_url: str, api_key: str) -> list[str]:
    """ä» OpenAI å…¼å®¹ API è·å–æ¨¡å‹åˆ—è¡¨"""
    url = f"{base_url.rstrip('/')}/models"
    headers = {"Authorization": f"Bearer {api_key}"}

    async with aiohttp.ClientSession() as session:
        async with session.get(url, headers=headers, timeout=10) as resp:
            if resp.status == 200:
                data = await resp.json()
                models = [m["id"] for m in data.get("data", [])]
                # è¿‡æ»¤å‡º chat æ¨¡å‹
                chat_models = [m for m in models if any(
                    x in m.lower() for x in ["gpt", "chat", "claude", "llama", "qwen", "deepseek"]
                )]
                return sorted(chat_models) if chat_models else sorted(models)
            return []


async def fetch_ollama_models(base_url: str) -> list[str]:
    """ä» Ollama è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨"""
    url = f"{base_url.rstrip('/')}/api/tags"

    async with aiohttp.ClientSession() as session:
        async with session.get(url, timeout=10) as resp:
            if resp.status == 200:
                data = await resp.json()
                return [m["name"] for m in data.get("models", [])]
            return []
```

### 6.5 ç³»ç»Ÿæç¤ºè¯

```python
# speaky/llm/prompts.py

AGENT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ¡Œé¢è¯­éŸ³åŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šç”¨è¯­éŸ³ç»™ä½ æŒ‡ä»¤ï¼Œä½ éœ€è¦ç†è§£æ„å›¾å¹¶ä½¿ç”¨å¯ç”¨çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚

## å·¥ä½œåŸåˆ™

1. ç†è§£ç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œå³ä½¿è¡¨è¿°ä¸å¤Ÿç²¾ç¡®
2. ä¼˜å…ˆä½¿ç”¨å·¥å…·å®Œæˆä»»åŠ¡ï¼Œè€Œä¸æ˜¯åªç»™å»ºè®®
3. å¦‚æœä»»åŠ¡æ— æ³•å®Œæˆï¼Œæ¸…æ™°è¯´æ˜åŸå› 
4. å›å¤ç®€æ´ï¼Œé€‚åˆè¯­éŸ³æ’­æŠ¥ï¼ˆæ§åˆ¶åœ¨ 50 å­—ä»¥å†…ï¼‰

## å¸¸è§ä»»åŠ¡ç¤ºä¾‹

- "æ‰“å¼€ GitHub" â†’ ä½¿ç”¨æµè§ˆå™¨å·¥å…·æ‰“å¼€ github.com
- "æœç´¢ Python æ•™ç¨‹" â†’ ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€æœç´¢é¡µé¢
- "è¯»å–æ¡Œé¢ä¸Šçš„ readme æ–‡ä»¶" â†’ ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿå·¥å…·è¯»å–æ–‡ä»¶
- "å¸®æˆ‘åˆ›å»ºä¸€ä¸ªç¬”è®°" â†’ ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿå·¥å…·åˆ›å»ºæ–‡ä»¶

## æ³¨æ„äº‹é¡¹

- å¦‚æœæ²¡æœ‰åˆé€‚çš„å·¥å…·ï¼Œç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
- å·¥å…·è°ƒç”¨å¤±è´¥æ—¶ï¼Œå‘ŠçŸ¥ç”¨æˆ·åŸå› å¹¶æä¾›æ›¿ä»£æ–¹æ¡ˆ
"""
```

---

## å…­ã€Handler é›†æˆ

### 6.1 LLMAgentHandler

```python
# speaky/handlers/llm_agent.py

import asyncio
import logging
import threading
from typing import Optional

from PySide6.QtCore import QTimer

from ..llm.client import LLMClient

logger = logging.getLogger(__name__)


class LLMAgentHandler:
    """LLM Agent æ¨¡å¼å¤„ç†å™¨ï¼ˆåŸºäº LangChainï¼‰"""

    def __init__(self, signals, recorder, engine_getter, floating_window, config):
        self._signals = signals
        self._recorder = recorder
        self._engine_getter = engine_getter
        self._floating_window = floating_window
        self._config = config

        self._llm_client: Optional[LLMClient] = None
        self._is_recording = False
        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ– LLM Client å’Œ MCP å·¥å…·"""
        if self._initialized:
            return

        self._llm_client = LLMClient(self._config.get("llm", {}))

        # è·å– MCP Server é…ç½®å¹¶åˆå§‹åŒ–
        mcp_servers = self._config.get("mcp", {}).get("servers", {})
        await self._llm_client.initialize(mcp_servers)

        self._initialized = True
        logger.info("LLM Agent initialized with MCP tools")

    def on_hotkey_press(self):
        """å¿«æ·é”®æŒ‰ä¸‹ - å¼€å§‹å½•éŸ³"""
        if not self._config.get("core.llm_agent.enabled", True):
            return

        self._is_recording = True
        self._floating_window.show()
        self._floating_window.set_status("listening")
        self._floating_window.set_text("æ­£åœ¨è†å¬...")
        self._recorder.start()

    def on_hotkey_release(self):
        """å¿«æ·é”®é‡Šæ”¾ - åœæ­¢å½•éŸ³å¹¶å¤„ç†"""
        if not self._is_recording:
            return

        self._is_recording = False
        audio_data = self._recorder.stop()

        # å¼‚æ­¥å¤„ç†
        threading.Thread(
            target=self._process_audio,
            args=(audio_data,),
            daemon=True
        ).start()

    def _process_audio(self, audio_data):
        """å¤„ç†éŸ³é¢‘ï¼šASR â†’ LangChain Agent â†’ è¿”å›ç»“æœ"""
        try:
            # 0. ç¡®ä¿å·²åˆå§‹åŒ–
            if not self._initialized:
                asyncio.run(self.initialize())

            # 1. è¯­éŸ³è¯†åˆ«
            self._signals.partial_result.emit("æ­£åœ¨è¯†åˆ«è¯­éŸ³...")
            engine = self._engine_getter()
            text = engine.recognize(audio_data)

            if not text:
                self._signals.partial_result.emit("æœªè¯†åˆ«åˆ°è¯­éŸ³")
                QTimer.singleShot(2000, self._floating_window.hide)
                return

            self._signals.partial_result.emit(f"ğŸ¤ {text}\n\nâ³ æ­£åœ¨å¤„ç†...")

            # 2. LangChain Agent å¤„ç†ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
            result = asyncio.run(self._run_agent(text))

            # 3. æ˜¾ç¤ºç»“æœ
            self._signals.partial_result.emit(f"ğŸ¤ {text}\n\nâœ… {result}")

        except Exception as e:
            logger.error(f"LLM Agent error: {e}", exc_info=True)
            self._signals.partial_result.emit(f"âŒ é”™è¯¯: {e}")
        finally:
            # å»¶è¿Ÿéšè—çª—å£
            QTimer.singleShot(3000, self._floating_window.hide)

    async def _run_agent(self, text: str) -> str:
        """è¿è¡Œ LangChain Agent"""
        return await self._llm_client.chat(text)

    async def _run_agent_stream(self, text: str):
        """è¿è¡Œ LangChain Agentï¼ˆæµå¼ï¼‰"""
        full_response = ""
        async for chunk in self._llm_client.chat_stream(text):
            full_response += chunk
            self._signals.partial_result.emit(f"ğŸ¤ {text}\n\n{full_response}")
        return full_response
```

---

## ä¸ƒã€å®ç°è®¡åˆ’

### P0 - åŸºç¡€åŠŸèƒ½ï¼ˆå¿…é¡»ï¼‰

1. **é…ç½®ç»“æ„**
   - åœ¨ `config.py` æ·»åŠ  `core.llm_agent`ã€`llm`ã€`mcp` é…ç½®é¡¹

2. **è®¾ç½®ç•Œé¢**
   - æ–°å¢ `LLMAgentPage` é¡µé¢
   - LLM é…ç½®ï¼šBase URL / API Key / Modelï¼ˆè‡ªåŠ¨æ‹‰å–ï¼‰
   - MCP é…ç½®ï¼šServer åˆ—è¡¨ã€æ·»åŠ /ç¼–è¾‘/åˆ é™¤

3. **LLM å®¢æˆ·ç«¯**
   - æ”¯æŒ OpenAI å…¼å®¹ API
   - æ”¯æŒ Tool Callingï¼ˆfunction callingï¼‰
   - å®ç° `fetch_models()` æ‹‰å–æ¨¡å‹åˆ—è¡¨

4. **MCP å®¢æˆ·ç«¯**
   - å®ç° `MCPManager` ç®¡ç†å¤šä¸ª Server
   - æ”¯æŒ stdio ä¼ è¾“æ–¹å¼
   - å·¥å…·åˆ—è¡¨è·å–å’Œè°ƒç”¨

5. **Agent Handler**
   - å¿«æ·é”®ç›‘å¬ï¼ˆé»˜è®¤ Tabï¼‰
   - è¯­éŸ³è¯†åˆ« â†’ LLM + MCP â†’ æ˜¾ç¤ºç»“æœ

### P1 - æ‰©å±•åŠŸèƒ½

1. **Ollama æ”¯æŒ**
   - æœ¬åœ°æ¨¡å‹è°ƒç”¨ï¼ˆéœ€è¦æ”¯æŒ Tool Calling çš„æ¨¡å‹ï¼‰

2. **MCP Server ç®¡ç†**
   - æµ‹è¯•è¿æ¥åŠŸèƒ½
   - æŸ¥çœ‹å¯ç”¨å·¥å…·åˆ—è¡¨

3. **æ‰§è¡Œå†å²**
   - ä¿å­˜æ‰§è¡Œå†å²
   - å¿«é€Ÿé‡å¤æ‰§è¡Œ

### P2 - é«˜çº§åŠŸèƒ½

1. **å¤šè½®å¯¹è¯**
   - æ”¯æŒè¿½é—®å’Œæ¾„æ¸…

2. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**
   - ç»“åˆå½“å‰ç„¦ç‚¹åº”ç”¨æä¾›ä¸Šä¸‹æ–‡

3. **è‡ªå®šä¹‰ MCP Server**
   - æä¾›æ¨¡æ¿å¿«é€Ÿåˆ›å»ºè‡ªå®šä¹‰ Server

---

## ä¹ã€æ–‡ä»¶ç»“æ„

```
speaky/
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py           # LangChain LLM å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ models.py           # æ¨¡å‹åˆ—è¡¨è·å–
â”‚   â”œâ”€â”€ prompts.py          # ç³»ç»Ÿæç¤ºè¯
â”‚   â””â”€â”€ types.py            # AgentContent, ToolCall ç­‰æ•°æ®ç»“æ„
â”œâ”€â”€ handlers/
â”‚   â”œâ”€â”€ llm_agent.py        # LLM Agent Handlerï¼ˆæ–°å¢ï¼‰
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ floating_window.py  # æ‰©å±•æ”¯æŒ Agent æ¨¡å¼æ˜¾ç¤º
â”‚   â”œâ”€â”€ settings_dialog.py  # æ·»åŠ  LLMAgentPage
â”‚   â”œâ”€â”€ mcp_server_dialog.py # MCP Server ç¼–è¾‘å¯¹è¯æ¡†ï¼ˆæ–°å¢ï¼‰
â”‚   â””â”€â”€ ...
â””â”€â”€ config.py               # æ·»åŠ  llm_agent / llm / mcp é…ç½®
```

---

## åã€ä¾èµ–

```toml
# pyproject.toml æ–°å¢ä¾èµ–
[project]
dependencies = [
    # ... ç°æœ‰ä¾èµ–

    # LangChain æ ¸å¿ƒ
    "langchain>=0.3.0",
    "langchain-core>=0.3.0",

    # LLM æä¾›å•†
    "langchain-openai>=0.2.0",      # OpenAI / å…¼å®¹ API
    "langchain-ollama>=0.2.0",      # Ollama æœ¬åœ°æ¨¡å‹

    # MCP é›†æˆ
    "langchain-mcp-adapters>=0.1.0", # MCP å·¥å…·é€‚é…å™¨
    "mcp>=1.0.0",                    # MCP Python SDK

    # å¼‚æ­¥ HTTP
    "aiohttp>=3.9.0",
]
```

### ä¾èµ–è¯´æ˜

| åŒ…å | ç”¨é€” |
|------|------|
| `langchain` | LangChain æ ¸å¿ƒæ¡†æ¶ |
| `langchain-openai` | OpenAI ChatGPT æ”¯æŒ |
| `langchain-ollama` | Ollama æœ¬åœ°æ¨¡å‹æ”¯æŒ |
| `langchain-mcp-adapters` | å°† MCP å·¥å…·è½¬æ¢ä¸º LangChain Tools |
| `mcp` | MCP åè®® Python SDK |
| `aiohttp` | å¼‚æ­¥ HTTP å®¢æˆ·ç«¯ï¼ˆè·å–æ¨¡å‹åˆ—è¡¨ç­‰ï¼‰ |

---

## åä¸€ã€å®‰å…¨è€ƒè™‘

1. **MCP Server æƒé™æ§åˆ¶**
   - æ–‡ä»¶ç³»ç»Ÿ Server é™åˆ¶è®¿é—®ç›®å½•
   - æ•æ„Ÿæ“ä½œéœ€è¦ç”¨æˆ·ç¡®è®¤

2. **API Key å®‰å…¨**
   - å¯†ç è¾“å…¥æ¡†éšè—æ˜¾ç¤º
   - é…ç½®æ–‡ä»¶æƒé™æ§åˆ¶ï¼ˆ600ï¼‰

3. **å·¥å…·è°ƒç”¨å®¡è®¡**
   - è®°å½•æ‰€æœ‰å·¥å…·è°ƒç”¨æ—¥å¿—
   - å¯æŸ¥çœ‹æ‰§è¡Œå†å²
